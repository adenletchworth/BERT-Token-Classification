{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER) with Transformers \n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, we will be doing Natural Language Processing with BERT Transformer models. Named Entity Recognition (NER) is a Token Classification task which identifies and extracts entites from text documents. \n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Understand Tokenizing Process \n",
    "- Go through NER pipeline\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "https://www.kaggle.com/datasets/naseralqaydeh/named-entity-recognition-ner-corpus/data - Kaggle Dataset for NER with Corresponding Entity Tags for each Sentence\n",
    "\n",
    "#### Attributes\n",
    "- Sentence # - Index (String)\n",
    "- Sentence - Text Data (String)\n",
    "- POS - Part of Speech (String)\n",
    "- Tag - Entity Tag (String)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizerFast, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "import ast\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Sentence #', 'Sentence', 'POS', 'Tag'], dtype='object') Sentence #    object\n",
      "Sentence      object\n",
      "POS           object\n",
      "Tag           object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Import Data\n",
    "df = pd.read_csv('./Data/ner.csv')\n",
    "\n",
    "print(df.columns, df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence #    0\n",
       "Sentence      0\n",
       "POS           0\n",
       "Tag           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP'...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Families of soldiers killed in the conflict jo...</td>\n",
       "      <td>['NNS', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'NN', ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 3</td>\n",
       "      <td>They marched from the Houses of Parliament to ...</td>\n",
       "      <td>['PRP', 'VBD', 'IN', 'DT', 'NNS', 'IN', 'NN', ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 4</td>\n",
       "      <td>Police put the number of marchers at 10,000 wh...</td>\n",
       "      <td>['NNS', 'VBD', 'DT', 'NN', 'IN', 'NNS', 'IN', ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 5</td>\n",
       "      <td>The protest comes on the eve of the annual con...</td>\n",
       "      <td>['DT', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'D...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #                                           Sentence  \\\n",
       "0  Sentence: 1  Thousands of demonstrators have marched throug...   \n",
       "1  Sentence: 2  Families of soldiers killed in the conflict jo...   \n",
       "2  Sentence: 3  They marched from the Houses of Parliament to ...   \n",
       "3  Sentence: 4  Police put the number of marchers at 10,000 wh...   \n",
       "4  Sentence: 5  The protest comes on the eve of the annual con...   \n",
       "\n",
       "                                                 POS  \\\n",
       "0  ['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP'...   \n",
       "1  ['NNS', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'NN', ...   \n",
       "2  ['PRP', 'VBD', 'IN', 'DT', 'NNS', 'IN', 'NN', ...   \n",
       "3  ['NNS', 'VBD', 'DT', 'NN', 'IN', 'NNS', 'IN', ...   \n",
       "4  ['DT', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'D...   \n",
       "\n",
       "                                                 Tag  \n",
       "0  ['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', '...  \n",
       "1  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "2  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "3  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "4  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>47959</td>\n",
       "      <td>47959</td>\n",
       "      <td>47959</td>\n",
       "      <td>47959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>47959</td>\n",
       "      <td>47575</td>\n",
       "      <td>47214</td>\n",
       "      <td>33318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>VOA 's Mil Arcega reports .</td>\n",
       "      <td>['NNP', 'POS', 'NNP', 'NNP', 'VBZ', '.']</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>39</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Sentence #                     Sentence  \\\n",
       "count             47959                        47959   \n",
       "unique            47959                        47575   \n",
       "top     Sentence: 47959  VOA 's Mil Arcega reports .   \n",
       "freq                  1                           17   \n",
       "\n",
       "                                             POS  \\\n",
       "count                                      47959   \n",
       "unique                                     47214   \n",
       "top     ['NNP', 'POS', 'NNP', 'NNP', 'VBZ', '.']   \n",
       "freq                                          39   \n",
       "\n",
       "                                                      Tag  \n",
       "count                                               47959  \n",
       "unique                                              33318  \n",
       "top     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "freq                                                  450  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the string representation of a list to a list\n",
    "df['Tag'] = df['Tag'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-art',\n",
       " 'B-eve',\n",
       " 'B-geo',\n",
       " 'B-gpe',\n",
       " 'B-nat',\n",
       " 'B-org',\n",
       " 'B-per',\n",
       " 'B-tim',\n",
       " 'I-art',\n",
       " 'I-eve',\n",
       " 'I-geo',\n",
       " 'I-gpe',\n",
       " 'I-nat',\n",
       " 'I-org',\n",
       " 'I-per',\n",
       " 'I-tim',\n",
       " 'O'}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Atomize tags with explode and get unique labels\n",
    "labels = set(df['Tag'].explode().unique()) \n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Tags\n",
    "\n",
    "### Prefixes (Chunks)\n",
    "\n",
    "`B` - prefix indicates the beginning of a named entity. <br>\n",
    "`I` - prefix indicates that the token is inside a named entity. <br>\n",
    "`O` - indicates that the token is not a named entity. <br>\n",
    "<br>\n",
    "\n",
    "### Suffixes\n",
    "`art` Artifacts, e.g., books, songs, etc.<br>\n",
    "`eve` Events, e.g., battles, elections, holidays, etc.<br>\n",
    "`geo` Geographical entities, e.g., cities, rivers, countries, etc.<br>\n",
    "`gpe` Geopolitical entities, e.g., cities, states, countries.<br>\n",
    "`nat` Natural phenomena, e.g., hurricanes, earthquakes.<br>\n",
    "`org` Organizations, e.g., companies, government organizations, etc.<br>\n",
    "`per` Persons.<br>\n",
    "`tim` Time indicators, e.g., dates, days, months, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I-tim': 0,\n",
       " 'O': 1,\n",
       " 'B-art': 2,\n",
       " 'B-eve': 3,\n",
       " 'I-gpe': 4,\n",
       " 'B-org': 5,\n",
       " 'I-per': 6,\n",
       " 'B-tim': 7,\n",
       " 'I-geo': 8,\n",
       " 'I-nat': 9,\n",
       " 'B-gpe': 10,\n",
       " 'I-eve': 11,\n",
       " 'B-nat': 12,\n",
       " 'B-geo': 13,\n",
       " 'I-org': 14,\n",
       " 'I-art': 15,\n",
       " 'B-per': 16}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_id = {l: i for i, l in enumerate(labels)}\n",
    "id_to_label = {i: l for l, i in label_to_id.items()}\n",
    "\n",
    "label_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>[O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Families of soldiers killed in the conflict jo...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They marched from the Houses of Parliament to ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-geo, I-geo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Police put the number of marchers at 10,000 wh...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The protest comes on the eve of the annual con...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-geo, O, O,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0  Thousands of demonstrators have marched throug...   \n",
       "1  Families of soldiers killed in the conflict jo...   \n",
       "2  They marched from the Houses of Parliament to ...   \n",
       "3  Police put the number of marchers at 10,000 wh...   \n",
       "4  The protest comes on the eve of the annual con...   \n",
       "\n",
       "                                                 Tag  \n",
       "0  [O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, B-geo, I-geo...  \n",
       "3      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, B-geo, O, O,...  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Isolate the sentence and tag columns\n",
    "df = df[['Sentence', 'Tag']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "sentence = df['Sentence'].iloc[0]\n",
    "\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer \n",
    "\n",
    "### Input\n",
    "\n",
    "Input sequences are expected to be a string sequence of words in order to tokenize them. The max length the tokenizer can handle is 512, therefore sequences over will be truncated.\n",
    "\n",
    "### Tokenizer Parameters\n",
    "\n",
    "`add_special_tokens` : Automatically adds **[CLS]** and **[SEP]** tokens\n",
    "\n",
    "`padding` : If sequence length not reach maximum add **[PAD]** token\n",
    "\n",
    "`max_length` : maximum sequence length in tokens\n",
    "\n",
    "`truncation` : truncate sequence if it exceeds max_length\n",
    "\n",
    "`return_tensors` : tensor return type\n",
    "\n",
    "\n",
    "### Special Tokens\n",
    "\n",
    "**[CLS]** - Classifier tokens, Tells our model that this is the start of the sequence\n",
    "\n",
    "**[SEP]** - Seperator token, Indicates end of sequence, used for others tasks such as QA\n",
    "\n",
    "**[PAD]** - Padding Token for ensuring all sequences are the same length if under max length\n",
    "\n",
    "\n",
    "### Outputs\n",
    "\n",
    "`input_ids` : numeric represnetation of tokens, where {101: **[CLS]**, 102: **[SEP]**, 0: **[PAD]** }\n",
    "\n",
    "`token_type_ids` : numeric representation of sequence, used in sequence classification or question answering \n",
    "\n",
    "`attention_mask` : Boolean for not **[PAD]** token, that is 1 for real tokens, else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([None,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None],\n",
       " ['[CLS]',\n",
       "  'thousands',\n",
       "  'of',\n",
       "  'demonstrators',\n",
       "  'have',\n",
       "  'marched',\n",
       "  'through',\n",
       "  'london',\n",
       "  'to',\n",
       "  'protest',\n",
       "  'the',\n",
       "  'war',\n",
       "  'in',\n",
       "  'iraq',\n",
       "  'and',\n",
       "  'demand',\n",
       "  'the',\n",
       "  'withdrawal',\n",
       "  'of',\n",
       "  'british',\n",
       "  'troops',\n",
       "  'from',\n",
       "  'that',\n",
       "  'country',\n",
       "  '.',\n",
       "  '[SEP]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]'],\n",
       " {'input_ids': tensor([[  101,  5190,  1997, 28337,  2031,  9847,  2083,  2414,  2000,  6186,\n",
       "           1996,  2162,  1999,  5712,  1998,  5157,  1996, 10534,  1997,  2329,\n",
       "           3629,  2013,  2008,  2406,  1012,   102,     0,     0,     0,     0,\n",
       "              0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 0, 0, 0, 0, 0, 0]])})"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input = tokenizer(sentence, add_special_tokens=True, padding='max_length', truncation=True, max_length=32, return_tensors='pt')\n",
    "\n",
    "tokenized_input\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input['input_ids'][0])\n",
    "\n",
    "word_ids = tokenized_input.word_ids()\n",
    "\n",
    "word_ids, tokens, tokenized_input,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 24)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens), len(word_ids), len(df['Tag'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((38367, 2), (5755, 2), (3837, 2))"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into train, test, and validation sets\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=2002)\n",
    "test, val = train_test_split(test, test_size=0.4, random_state=2002)\n",
    "\n",
    "train.shape, test.shape, val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(text, label_list,label_to_id):\n",
    "    # Tokenize text\n",
    "    tokenized_input = tokenizer(text, add_special_tokens=True, truncation=True, max_length=32, padding='max_length', return_tensors='pt')\n",
    "    word_ids = tokenized_input.word_ids(batch_index=0)  # Assuming batch_size=1 for simplicity\n",
    "    \n",
    "    aligned_labels = []\n",
    "    prev_word_id = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:  # Special tokens\n",
    "            aligned_labels.append(-100)\n",
    "        elif word_id != prev_word_id:  # New word\n",
    "            if word_id < len(label_list):    \n",
    "                aligned_labels.append(label_to_id[label_list[word_id]])\n",
    "            else:\n",
    "                aligned_labels.append(-100)\n",
    "        else:  # Subword tokens\n",
    "            aligned_labels.append(-100)  # Same label as the first subword or ignore\n",
    "        prev_word_id = word_id\n",
    "\n",
    "    print(aligned_labels)\n",
    "    tokenized_input[\"labels\"] = torch.tensor([aligned_labels])  \n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 13, -100, -100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, -100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 28352, 15217, 12693,  2874,  6322,  3180,  8647,  1998,  5008,\n",
       "          4491,  1010,  2788, 11248,  2006, 28352, 10875, 17934, 17773,  1996,\n",
       "          2430,  2231,  2005,  2062, 12645,  1998,  1037,  3469,  3745,  1997,\n",
       "          1996,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[-100,   13, -100, -100,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    5, -100,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1, -100]])}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ids = tokenize_and_align_labels(train['Sentence'].iloc[0], train['Tag'].iloc[0], label_to_id= label_to_id)\n",
    "\n",
    "tokenized_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 13, -100, -100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, -100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100]\n",
      "Epoch: 0, Loss:  2.954951763153076\n",
      "[-100, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, -100, -100]\n",
      "[-100, 1, 1, 1, 1, -100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13, 8, 1, 1, 1, 1, 1, 1, 1, 10, 1, 16, 6, 1, 1, 1, -100]\n",
      "[-100, 5, 14, 14, 16, -100, -100, 6, 1, 1, 1, 1, 1, 1, 5, 14, 14, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13, -100, -100]\n",
      "[-100, 16, 13, 1, -100, -100, -100, -100, 1, 1, 1, 1, 1, 1, 1, 16, 6, 6, -100, 1, -100, -100, 1, 1, 7, 1, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 16, 6, 1, 1, 1, 1, -100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100]\n",
      "[-100, 16, 6, -100, 6, -100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 13, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 13, 1, 1, 1, 1, 1, 1, 1, 1, 16, 6, 1, 1, 1, 1, 5, 14, 14, 14, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 13, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 13, 1, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 10, 1, 1, 16, 6, -100, -100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13, 1, 1, 13, 1, 1, 1, -100, -100]\n",
      "[-100, 13, 1, 1, 1, 1, 1, 1, 1, 1, 13, 8, 1, 13, 1, 5, 14, 14, 14, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, -100, 1, 1, 1, 1, 1, 1, 10, 1, 1, 13, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 5, -100, -100, 1, 1, 13, 1, 1, 5, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 16, -100, -100, 1, 10, 1, 1, 1, 1, 1, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 13, 8, 1, 1, 1, 1, -100, -100, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 10, -100, 16, 6, 6, -100, -100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13, 8, 8, 8, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 13, 1, 1, 10, 1, 1, 10, -100, 1, 1, 10, -100, 1, 1, 10, -100, -100, -100, 1, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 16, 6, 6, 1, 5, 1, 16, 6, -100, -100, 6, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 10, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13, 8, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 7, 1, 1, 1, -100, 10, 16, 6, 6, -100, 1, -100, -100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 0, 1, 1, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 0, 1, 1, 1, 1, -100, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 5, 14, 1, 1, 1, 1, 1, 1, -100, -100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 10, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 1, 1, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 5, 14, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 13, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 7, 1, 1, 1, 1, 13, -100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100]\n",
      "[-100, 5, 14, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 16, -100, -100, 1, 1, 1, 1, 1, 7, 0, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 14, 16, 6, 1, 1, 1, 1, 7, 13, 1, 1, 1, -100]\n",
      "[-100, 16, -100, -100, 1, 1, 1, 10, 1, 16, 6, -100, -100, -100, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 10, 1, 1, 1, 1, 13, 10, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13, 1, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, 1, 16, 6, 1, 7, 0, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 16, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 5, 14, 1, 1, 1, 1, 1, 1, 1, 10, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 13, -100, -100, 1, 1, 1, 1, 1, 1, -100, 1, 7, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, -100, 1, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 10, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 7, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 10, 1, 1, 5, 14, 1, 13, 8, 1, 1, 1, 1, 1, 10, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, -100, -100, -100, 1, 13, 1, 1, 1, -100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 13, 8, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 5, 14, 1, 1, 7, -100, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100]\n",
      "[-100, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 7, 1, 1, 10, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13, 1, 1, 1, 1, 1, 1, 1, 13, 1, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 7, 1, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13, 1, -100, -100]\n",
      "[-100, 16, 6, 1, -100, 1, 1, 1, 1, 1, 1, 13, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 16, 6, 1, 1, 1, 10, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 16, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 7, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 13, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 5, 14, 14, 14, 1, 1, 1, 1, 1, 13, 1, 13, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 13, 8, 1, 13, 1, 1, -100]\n",
      "[-100, 16, 6, -100, -100, 1, 1, 1, 5, 1, 1, 1, 5, 14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 5, 1, 1, 16, 1, 10, 1, 1, 1, 1, 1, 1, 10, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 13, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 16, 1, 1, 5, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 13, 1, 1, 1, 1, 1, 1, 1, 1, -100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 13, 1, 16, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, 13, 1, 1, 13, 7, 1, 7, 0, 1, 1, 1, 1, 1, 1, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 13, 1, 1, 1, 1, -100, 16, 6, 6, 1, -100, -100, 1, 1, -100, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100]\n",
      "[-100, 10, 1, 1, 1, 1, 1, 1, 1, 1, 13, -100, 8, 1, 1, 7, 1, 1, 1, 13, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 16, 6, -100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 16, -100, 6, -100, -100, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 13, 1, 1, 1, 1, 1, 1, 1, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13, 1, 1, 1, -100, 1, 13, 1, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 5, -100, -100, 14, 14, 1, 1, 1, 1, 1, 1, 10, 1, 1, 1, 13, -100, 1, 7, 1, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 10, 1, 1, 1, 1, 1, 1, 1, 1, 10, 1, 1, 1, 13, 8, 1, 1, 1, 1, 1, 1, 1, 1, -100]\n",
      "[-100, 1, -100, 1, 7, 1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100]\n",
      "[-100, 1, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 10, 1, 10, 1, 1, 1, 1, 1, 1, 1, 1, 7, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 5, 14, -100, -100, 1, 1, 1, 13, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100]\n",
      "[-100, 16, 6, 6, 1, 1, 1, 1, 13, 8, 1, 1, 13, -100, 1, 13, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 13, -100, 1, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, -100, -100, -100, 1, 1, 1, 1, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100]\n",
      "[-100, 5, -100, 1, 1, 1, 1, 5, 14, 16, 6, 1, -100, 1, 1, 7, 1, 13, 8, 1, 13, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, -100, 1, 1, 1, 1, 1, 1, 1, -100, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 13, 1, 1, 1, 1, 1, 1, 1, 7, 1, 1, 1, -100, 1, -100, -100, 1, 16, 6, -100, 6, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 13, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 10, 5, 14, 14, 1, 1, 10, 5, 14, 14, 1, 1, 1, 1, 1, 7, 1, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 13, -100, 1, 1, 1, 1, 1, 1, 1, 7, 0, 0, 1, 7, 1, 7, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 5, 14, 14, 14, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 16, 6, 1, 1, 1, 1, 1, 1, 1, 16, 1, 1, -100, -100]\n",
      "[-100, 13, 1, 1, 1, 1, 5, 14, 1, 1, -100, 1, -100, -100, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 16, 6, 1, 1, 1, 1, 13, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13, 8, 1, 1, 1, -100]\n",
      "[-100, 1, 1, 1, 1, -100, 1, 1, 1, 1, 1, 10, 1, 7, 1, 1, 1, 1, 5, 14, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, -100, -100, 1, 1, -100, -100, -100, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 5, 14, 14, -100, 1, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "Epoch: 0, Loss:  0.9411158561706543\n",
      "[-100, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, -100, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 10, -100, 1, 1, -100, -100, -100, -100]\n",
      "[-100, 1, 13, 1, 1, 5, 1, 1, 2, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 16, 13, 1, -100, -100, 1, 1, 1, 1, 1, 1, 1, 7, 1, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 16, -100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13, 8, -100, 1, -100, -100]\n",
      "[-100, 16, 6, 1, 1, 1, 1, 1, 1, 1, 1, 10, 1, 1, 1, 1, 13, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 16, 6, 1, 1, 7, 0, 0, 1, 1, 1, -100, -100, 1, 1, 1, 1, 13, 8, 1, 1, 1, 1, 1, 1, 10, 1, 1, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, -100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 10, 16, 6, -100, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 5, 14, -100, 14, -100, -100, -100, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 5, 14, 1, 1, 1, 1, 16, 13, 1, -100, -100, 1, 7, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 13, 8, 1, 1, 13, 1, 1, 1, 1, -100, -100, -100, 1, 1, 1, 1, 10, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 7, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 16, 6, 1, 1, -100, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 16, 6, 1, -100, 1, 1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, -100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 10, 1, 1, 1, 1, -100, -100, -100, 1, 1, 1, 7, -100, 1, 1, 1, 1, 1, -100]\n",
      "[-100, 1, 3, 11, 11, 1, 16, 1, 1, 1, 1, 1, 1, 13, 1, 1, 1, 5, 1, 1, -100, 1, 1, 13, 1, 1, 1, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 13, 1, 1, 1, 1, 1, 7, 1, 13, -100, -100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100]\n",
      "[-100, 5, 14, 1, -100, 1, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, 1, 1, 1, -100, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 10, 1, 1, 16, 6, -100, -100, -100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, 1, -100, -100, -100, 1, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 13, 1, 1, 16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 13, 1, 1, 1, 1, 1, 1, 1, 10, 1, 1, 1, 1, 1, -100, -100, -100, 1, 7, 1, 1, 1, 1, 1, 1, 13, 1, 1, 1, 1, -100]\n",
      "[-100, 13, 1, 1, 1, 13, 1, 1, 1, 1, 1, 7, 1, 1, 1, 10, 1, 1, 1, 1, -100, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 7, 0, 1, 1, 1, 1, 1, 1, 16, 1, 1, 1, 1, 1, 1, 1, 1, 16, 6, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 10, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13, 1, -100, -100, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100]\n",
      "[-100, 5, -100, 13, 1, 16, 6, -100, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 13, 1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 13, -100, -100, 1, 1, 1, 1, 1, 1, 1, 10, 1, 1, 13, 1, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 7, 1, 1, 13, 8, 1, 1, 13, -100, 8, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 5, -100, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 13, 16, 1, 1, 1, 10, 1, 1, 13, 1, 13, 1, -100, 1, 7, -100, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 16, 13, 1, -100, 1, 1, 13, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, -100, 1, 1, 1, 13, -100, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 10, 1, 1, -100, -100, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 5, 1, 1, 1, 1, 1, 1, 1, 1, 13, -100, 1, 1, 1, 1, 1, -100, 1, 13, 1, 1, 1, 1, 1, 1, 7, 1, -100, -100, -100, -100]\n",
      "[-100, 16, 6, 1, -100, 1, 1, 1, 7, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 13, 1, 1, 7, 1, 1, 1, 7, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 5, 14, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 5, 14, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 16, 6, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100]\n",
      "[-100, 10, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13, 1, 1, -100, -100, 1, 1, 1, 1, 13, 1, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 13, 8, 1, 1, 1, 10, 1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 13, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 7, 1, 1, 1, 1, 1, 1, 10, 1, 1, 13, -100, -100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13, 1, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 13, 1, 1, 10, 1, 1, 1, 5, -100, 1, 1, 1, 1, 1, 1, 1, 1, -100, 1, 1, 1, 1, 1, 1, -100, 1, -100, -100, -100]\n",
      "[-100, 1, 5, 14, 1, 7, 1, 13, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 16, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 7, 1, 10, 1, 1, 1, 1, 1, 1, 1, 5, 14, 1, 1, 1, 1, 5, 14, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 13, 1, 13, 1, 1, 1, 1, 1, 13, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13, 8, 1, 1, 1, -100, -100, -100, 1, 10, 1, 1, -100]\n",
      "[-100, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13, -100, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, -100, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 13, 8, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[-100, 1, 1, 1, 1, 1, 5, 14, -100, 1, 7, 1, 1, 1, 1, -100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100]\n",
      "[-100, 16, 6, 1, 10, 16, 1, 16, -100, 6, -100, -100, 1, 1, 1, 1, 5, 14, 1, 1, 1, 1, 13, 1, 1, 1, 1, 13, 1, -100, -100, -100]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[188], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 18\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/optim/adam.py:393\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 393\u001b[0m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(labels))\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "metric = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "for epoch in range(3):\n",
    "    for i in range(len(train)):\n",
    "        optimizer.zero_grad()\n",
    "        tokenized_input = tokenize_and_align_labels(train['Sentence'].iloc[i], train['Tag'].iloc[i], label_to_id)\n",
    "        outputs = model(**tokenized_input)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for i in range(len(val)):\n",
    "    tokenized_input = tokenize_and_align_labels(val['Sentence'].iloc[i], val['Tag'].iloc[i], label_to_id)\n",
    "    outputs = model(**tokenized_input)\n",
    "    loss = outputs.loss\n",
    "    if i % 100 == 0:\n",
    "        print(f'Loss:  {loss.item()}')\n",
    "        \n",
    "model.save_pretrained('./models/ner_model')\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained('./models/ner_model')\n",
    "\n",
    "\n",
    "tokenized_input = tokenize_and_align_labels(val['Sentence'].iloc[0], val['Tag'].iloc[0], label_to_id)\n",
    "outputs = model(**tokenized_input)\n",
    "predicted_labels = outputs.logits.argmax(-1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
