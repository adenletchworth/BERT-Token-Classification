{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER) with Transformers \n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, we will be doing Natural Language Processing with BERT Transformer models. Named Entity Recognition (NER) is a Token Classification task which identifies and extracts entites from text documents. \n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Understand Tokenizing Process \n",
    "- Go through NER pipeline\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "https://www.kaggle.com/datasets/naseralqaydeh/named-entity-recognition-ner-corpus/data - Kaggle Dataset for NER with Corresponding Entity Tags for each Sentence\n",
    "\n",
    "#### Attributes\n",
    "- Sentence # - Index (String)\n",
    "- Sentence - Text Data (String)\n",
    "- POS - Part of Speech (String)\n",
    "- Tag - Entity Tag (String)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizerFast\n",
    "import ast\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Sentence #', 'Sentence', 'POS', 'Tag'], dtype='object') Sentence #    object\n",
      "Sentence      object\n",
      "POS           object\n",
      "Tag           object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Import Data\n",
    "df = pd.read_csv('./Data/ner.csv')\n",
    "\n",
    "print(df.columns, df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence #    0\n",
       "Sentence      0\n",
       "POS           0\n",
       "Tag           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP'...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Families of soldiers killed in the conflict jo...</td>\n",
       "      <td>['NNS', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'NN', ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 3</td>\n",
       "      <td>They marched from the Houses of Parliament to ...</td>\n",
       "      <td>['PRP', 'VBD', 'IN', 'DT', 'NNS', 'IN', 'NN', ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 4</td>\n",
       "      <td>Police put the number of marchers at 10,000 wh...</td>\n",
       "      <td>['NNS', 'VBD', 'DT', 'NN', 'IN', 'NNS', 'IN', ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 5</td>\n",
       "      <td>The protest comes on the eve of the annual con...</td>\n",
       "      <td>['DT', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'D...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #                                           Sentence  \\\n",
       "0  Sentence: 1  Thousands of demonstrators have marched throug...   \n",
       "1  Sentence: 2  Families of soldiers killed in the conflict jo...   \n",
       "2  Sentence: 3  They marched from the Houses of Parliament to ...   \n",
       "3  Sentence: 4  Police put the number of marchers at 10,000 wh...   \n",
       "4  Sentence: 5  The protest comes on the eve of the annual con...   \n",
       "\n",
       "                                                 POS  \\\n",
       "0  ['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP'...   \n",
       "1  ['NNS', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'NN', ...   \n",
       "2  ['PRP', 'VBD', 'IN', 'DT', 'NNS', 'IN', 'NN', ...   \n",
       "3  ['NNS', 'VBD', 'DT', 'NN', 'IN', 'NNS', 'IN', ...   \n",
       "4  ['DT', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'D...   \n",
       "\n",
       "                                                 Tag  \n",
       "0  ['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', '...  \n",
       "1  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "2  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "3  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "4  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>47959</td>\n",
       "      <td>47959</td>\n",
       "      <td>47959</td>\n",
       "      <td>47959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>47959</td>\n",
       "      <td>47575</td>\n",
       "      <td>47214</td>\n",
       "      <td>33318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>VOA 's Mil Arcega reports .</td>\n",
       "      <td>['NNP', 'POS', 'NNP', 'NNP', 'VBZ', '.']</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>39</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Sentence #                     Sentence  \\\n",
       "count             47959                        47959   \n",
       "unique            47959                        47575   \n",
       "top     Sentence: 47959  VOA 's Mil Arcega reports .   \n",
       "freq                  1                           17   \n",
       "\n",
       "                                             POS  \\\n",
       "count                                      47959   \n",
       "unique                                     47214   \n",
       "top     ['NNP', 'POS', 'NNP', 'NNP', 'VBZ', '.']   \n",
       "freq                                          39   \n",
       "\n",
       "                                                      Tag  \n",
       "count                                               47959  \n",
       "unique                                              33318  \n",
       "top     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "freq                                                  450  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the string representation of a list to a list\n",
    "df['Tag'] = df['Tag'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-art',\n",
       " 'B-eve',\n",
       " 'B-geo',\n",
       " 'B-gpe',\n",
       " 'B-nat',\n",
       " 'B-org',\n",
       " 'B-per',\n",
       " 'B-tim',\n",
       " 'I-art',\n",
       " 'I-eve',\n",
       " 'I-geo',\n",
       " 'I-gpe',\n",
       " 'I-nat',\n",
       " 'I-org',\n",
       " 'I-per',\n",
       " 'I-tim',\n",
       " 'O'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Atomize tags with explode and get unique labels\n",
    "labels = set((df['Tag'].explode().unique())) \n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Tags\n",
    "\n",
    "### Prefixes (Chunks)\n",
    "\n",
    "`B` - prefix indicates the beginning of a named entity. <br>\n",
    "`I` - prefix indicates that the token is inside a named entity. <br>\n",
    "`O` - indicates that the token is not a named entity. <br>\n",
    "<br>\n",
    "\n",
    "### Suffixes\n",
    "`art` Artifacts, e.g., books, songs, etc.<br>\n",
    "`eve` Events, e.g., battles, elections, holidays, etc.<br>\n",
    "`geo` Geographical entities, e.g., cities, rivers, countries, etc.<br>\n",
    "`gpe` Geopolitical entities, e.g., cities, states, countries.<br>\n",
    "`nat` Natural phenomena, e.g., hurricanes, earthquakes.<br>\n",
    "`org` Organizations, e.g., companies, government organizations, etc.<br>\n",
    "`per` Persons.<br>\n",
    "`tim` Time indicators, e.g., dates, days, months, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I-gpe': 0,\n",
       " 'I-per': 1,\n",
       " 'I-tim': 2,\n",
       " 'B-gpe': 3,\n",
       " 'B-org': 4,\n",
       " 'B-per': 5,\n",
       " 'B-nat': 6,\n",
       " 'I-art': 7,\n",
       " 'B-eve': 8,\n",
       " 'I-nat': 9,\n",
       " 'I-geo': 10,\n",
       " 'B-geo': 11,\n",
       " 'I-org': 12,\n",
       " 'B-art': 13,\n",
       " 'I-eve': 14,\n",
       " 'B-tim': 15,\n",
       " 'O': 16}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_id = {l: i for i, l in enumerate(labels)}\n",
    "id_to_label = {i: l for l, i in label_to_id.items()}\n",
    "\n",
    "label_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>[O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Families of soldiers killed in the conflict jo...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They marched from the Houses of Parliament to ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-geo, I-geo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Police put the number of marchers at 10,000 wh...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The protest comes on the eve of the annual con...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-geo, O, O,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0  Thousands of demonstrators have marched throug...   \n",
       "1  Families of soldiers killed in the conflict jo...   \n",
       "2  They marched from the Houses of Parliament to ...   \n",
       "3  Police put the number of marchers at 10,000 wh...   \n",
       "4  The protest comes on the eve of the annual con...   \n",
       "\n",
       "                                                 Tag  \n",
       "0  [O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, B-geo, I-geo...  \n",
       "3      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, B-geo, O, O,...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Isolate the sentence and tag columns\n",
    "df = df[['Sentence', 'Tag']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "sentence = df['Sentence'].iloc[0]\n",
    "\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer \n",
    "\n",
    "### Input\n",
    "\n",
    "Input sequences are expected to be a string sequence of words in order to tokenize them. The max length the tokenizer can handle is 512, therefore sequences over will be truncated.\n",
    "\n",
    "### Tokenizer Parameters\n",
    "\n",
    "`add_special_tokens` : Automatically adds **[CLS]** and **[SEP]** tokens\n",
    "\n",
    "`padding` : If sequence length not reach maximum add **[PAD]** token\n",
    "\n",
    "`max_length` : maximum sequence length in tokens\n",
    "\n",
    "`truncation` : truncate sequence if it exceeds max_length\n",
    "\n",
    "`return_tensors` : tensor return type\n",
    "\n",
    "\n",
    "### Special Tokens\n",
    "\n",
    "**[CLS]** - Classifier tokens, Tells our model that this is the start of the sequence\n",
    "\n",
    "**[SEP]** - Seperator token, Indicates end of sequence, used for others tasks such as QA\n",
    "\n",
    "**[PAD]** - Padding Token for ensuring all sequences are the same length if under max length\n",
    "\n",
    "\n",
    "### Outputs\n",
    "\n",
    "`input_ids` : numeric represnetation of tokens, where {101: **[CLS]**, 102: **[SEP]**, 0: **[PAD]** }\n",
    "\n",
    "`token_type_ids` : numeric representation of sequence, used in sequence classification or question answering \n",
    "\n",
    "`attention_mask` : Boolean for not **[PAD]** token, that is 1 for real tokens, else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([None,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None],\n",
       " ['[CLS]',\n",
       "  'thousands',\n",
       "  'of',\n",
       "  'demonstrators',\n",
       "  'have',\n",
       "  'marched',\n",
       "  'through',\n",
       "  'london',\n",
       "  'to',\n",
       "  'protest',\n",
       "  'the',\n",
       "  'war',\n",
       "  'in',\n",
       "  'iraq',\n",
       "  'and',\n",
       "  'demand',\n",
       "  'the',\n",
       "  'withdrawal',\n",
       "  'of',\n",
       "  'british',\n",
       "  'troops',\n",
       "  'from',\n",
       "  'that',\n",
       "  'country',\n",
       "  '.',\n",
       "  '[SEP]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]'],\n",
       " {'input_ids': tensor([[  101,  5190,  1997, 28337,  2031,  9847,  2083,  2414,  2000,  6186,\n",
       "           1996,  2162,  1999,  5712,  1998,  5157,  1996, 10534,  1997,  2329,\n",
       "           3629,  2013,  2008,  2406,  1012,   102,     0,     0,     0,     0,\n",
       "              0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 0, 0, 0, 0, 0, 0]])})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input = tokenizer(sentence, add_special_tokens=True, padding='max_length', truncation=True, max_length=32, return_tensors='pt')\n",
    "\n",
    "tokenized_input\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input['input_ids'][0])\n",
    "\n",
    "word_ids = tokenized_input.word_ids()\n",
    "\n",
    "word_ids, tokens, tokenized_input,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 24)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens), len(word_ids), len(df['Tag'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(text, label_list,label_to_id, tokenizer):\n",
    "    # Tokenize text\n",
    "    tokenized_input = tokenizer(text, add_special_tokens=True, truncation=True, max_length=32, padding='max_length', return_tensors='pt')\n",
    "    word_ids = tokenized_input.word_ids(batch_index=0)  # Assuming batch_size=1 for simplicity\n",
    "    \n",
    "    aligned_labels = []\n",
    "    prev_word_id = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:  # Special tokens\n",
    "            aligned_labels.append(-100)\n",
    "        elif word_id != prev_word_id:  # New word\n",
    "            if word_id < len(label_list):    \n",
    "                aligned_labels.append(label_to_id[label_list[word_id]])\n",
    "            else:\n",
    "                aligned_labels.append(-100)\n",
    "        else:  # Subword tokens\n",
    "            aligned_labels.append(-100)  # Same label as the first subword or ignore\n",
    "        prev_word_id = word_id\n",
    "\n",
    "    print(aligned_labels)\n",
    "    tokenized_input[\"labels\"] = torch.tensor([aligned_labels]) \n",
    "    \n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.data['Sentence'].iloc[idx]\n",
    "        labels = self.data['Tag'].iloc[idx]\n",
    "        encoding = tokenize_and_align_labels(sentence, labels, label_to_id, self.tokenizer)\n",
    "        return encoding\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((38367, 2), (5755, 2), (3837, 2))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into train, test, and validation sets\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=2002)\n",
    "test, val = train_test_split(test, test_size=0.4, random_state=2002)\n",
    "\n",
    "train.shape, test.shape, val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 11, -100, -100, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 4, -100, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, -100]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 28352, 15217, 12693,  2874,  6322,  3180,  8647,  1998,  5008,\n",
       "          4491,  1010,  2788, 11248,  2006, 28352, 10875, 17934, 17773,  1996,\n",
       "          2430,  2231,  2005,  2062, 12645,  1998,  1037,  3469,  3745,  1997,\n",
       "          1996,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[-100,   11, -100, -100,   16,   16,   16,   16,   16,   16,   16,   16,\n",
       "           16,   16,   16,    4, -100,   16,   16,   16,   16,   16,   16,   16,\n",
       "           16,   16,   16,   16,   16,   16,   16, -100]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training = dataset(train, tokenizer, 32)\n",
    "testing = dataset(test, tokenizer, 32)\n",
    "\n",
    "training[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 11, -100, -100, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 4, -100, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, -100]\n",
      "[-100, 11, -100, -100, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 4, -100, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, -100]\n",
      "bal B-geo\n",
      "province O\n",
      "experiences O\n",
      "regular O\n",
      "bombing O\n",
      "and O\n",
      "shooting O\n",
      "attacks O\n",
      ", O\n",
      "usually O\n",
      "blamed O\n",
      "on O\n",
      "bal B-org\n",
      "nationalists O\n",
      "battling O\n",
      "the O\n",
      "central O\n",
      "government O\n",
      "for O\n",
      "more O\n",
      "autonomy O\n",
      "and O\n",
      "a O\n",
      "larger O\n",
      "share O\n",
      "of O\n",
      "the O\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(tokenizer.convert_ids_to_tokens(training[0]['input_ids'][0]), training[0]['labels'][0]):\n",
    "    if label != -100:\n",
    "        print(token, id_to_label[label.item()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'TRAIN_BATCH_SIZE': 5,\n",
    "    'VALID_BATCH_SIZE': 2,\n",
    "    'EPOCHS': 1,\n",
    "    'LEARNING_RATE': 1e-5,\n",
    "    'MAX_LEN': 128,\n",
    "    'MAX_GRAD_NORM': 10\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    'batch_size': params['TRAIN_BATCH_SIZE'],\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0\n",
    "}\n",
    "\n",
    "test_params = {\n",
    "    'batch_size': params['VALID_BATCH_SIZE'],\n",
    "    'shuffle': False,\n",
    "    'num_workers': 0\n",
    "}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(training, **train_params)\n",
    "test_loader = torch.utils.data.DataLoader(testing, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(id_to_label), id2label=id_to_label, label2id=label_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 11, -100, -100, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 4, -100, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, -100]\n",
      "[-100, 11, -100, -100, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 4, -100, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, -100]\n",
      "[-100, 11, -100, -100, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 4, -100, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, -100]\n",
      "input_ids shape: torch.Size([1, 32])\n",
      "attention_mask shape: torch.Size([1, 32])\n",
      "labels shape: torch.Size([1, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.0552, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = training[0]['input_ids']\n",
    "attention_mask = training[0]['attention_mask']\n",
    "labels = training[0]['labels']\n",
    "print(\"input_ids shape:\", input_ids.shape)\n",
    "print(\"attention_mask shape:\", attention_mask.shape)\n",
    "print(\"labels shape:\", labels.shape)\n",
    "\n",
    "outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "loss= outputs.loss\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_logits shape: torch.Size([1, 32, 17])\n"
     ]
    }
   ],
   "source": [
    "training_logits = outputs.logits\n",
    "print(\"training_logits shape:\", training_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=params['LEARNING_RATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Function to train the model, given an epoch\n",
    "def train(epoch):\n",
    "\n",
    "    # Track Loss and Accuracy\n",
    "    loss, accuracy = 0, 0\n",
    "\n",
    "    # Track steps and examples \n",
    "    num_examples, num_steps = 0, 0\n",
    "\n",
    "    # Store predictions and labels\n",
    "    pred, label = [], []\n",
    "\n",
    "    model.train() # Flag the model to train\n",
    "\n",
    "    # Iterate over the training data from our DataLoader\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "\n",
    "        # Get our mask, input_ids, and labels from the batch\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        # Feed the input_ids, attention_mask, and labels to the model, then get loss and logits\n",
    "        outputs = model(input_ids=input_ids.squeeze(), attention_mask=attention_mask.squeeze(), labels=labels.squeeze())\n",
    "        loss, logits = outputs.loss, outputs.logits\n",
    "        loss += loss.item()\n",
    "\n",
    "        # Increment the number of steps and examples\n",
    "        num_steps += 1\n",
    "        num_examples += input_ids.size(0)\n",
    "\n",
    "        # Every 100 steps, print the loss\n",
    "        if idx % 100==0:\n",
    "            loss_step = loss / num_steps\n",
    "            print(f\"Epoch {epoch}, Step {idx}, Loss {loss_step:.4f}\")\n",
    "        \n",
    "        # Flatten the labels, logits, and predictions\n",
    "        flat_labels = labels.view(-1)\n",
    "        flat_logits = logits.view(-1, logits.size(-1))\n",
    "        flat_preds = torch.argmax(flat_logits, dim=-1)\n",
    "        \n",
    "        # Calculate the accuracy\n",
    "        active_acc = (flat_labels != -100)\n",
    "        accuracy += ((flat_preds == flat_labels) & active_acc).sum().item()\n",
    "        \n",
    "        # Store the predictions and labels\n",
    "        pred.extend(flat_preds[active_acc].cpu().numpy())\n",
    "        label.extend(flat_labels[active_acc].cpu().numpy())\n",
    "        \n",
    "        # Get the gradients and clip them\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=params['MAX_GRAD_NORM'])\n",
    "\n",
    "        # Zero the gradients, perform a backward pass, and update the weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    epoch_loss = loss / num_steps\n",
    "    epoch_accuracy = accuracy / num_examples\n",
    "    print(f\"Epoch {epoch}, Loss {epoch_loss:.4f}, Accuracy {epoch_accuracy:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainining' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCHS\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m# Flag the model to train\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Iterate over the training data from our DataLoader\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtrainining\u001b[49m):\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Get our mask, input_ids, and labels from the batch\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     20\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainining' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(params['EPOCHS']):\n",
    "    train(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "def validate(model, test_loader):\n",
    "    model.eval() # Flag the model to evaluate\n",
    "\n",
    "    # Same as before, track loss and accuracy, and store predictions and labels\n",
    "    loss, accuracy = 0, 0\n",
    "    num_examples, num_steps = 0, 0\n",
    "    pred, label = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(test_loader):\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "\n",
    "            outputs = model(input_ids=input_ids.squeeze(), attention_mask=attention_mask.squeeze(), labels=labels.squeeze())\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "            loss += loss.item()\n",
    "            num_steps += 1\n",
    "            num_examples += input_ids.size(0)\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                loss_step = loss / num_steps\n",
    "                print(f\"Step {idx}, Loss {loss_step:.4f}\")\n",
    "\n",
    "            flat_labels = labels.view(-1)\n",
    "            flat_logits = logits.view(-1, logits.size(-1))\n",
    "            flat_preds = torch.argmax(flat_logits, dim=-1)\n",
    "\n",
    "            active_acc = (flat_labels != -100)\n",
    "            accuracy += ((flat_preds == flat_labels) & active_acc).sum().item()\n",
    "            targets = torch.masked_select(flat_labels, active_acc)\n",
    "\n",
    "            pred.extend(flat_preds[active_acc])\n",
    "            label.extend(targets)\n",
    "    labels = [id_to_label[i] for i in label]\n",
    "    predictions = [id_to_label[i] for i in pred]\n",
    "\n",
    "    print(labels)\n",
    "    print(predictions)\n",
    "\n",
    "    epoch_loss = loss / num_steps\n",
    "    epoch_accuracy = accuracy / num_examples\n",
    "    print(f\"Loss {epoch_loss:.4f}, Accuracy {epoch_accuracy:.4f}\")\n",
    "    \n",
    "    return labels, predictions\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "labels, predictions = validate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(labels, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
